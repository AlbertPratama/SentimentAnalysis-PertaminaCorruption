{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library yang diperlukan\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset\n",
    "ds = load_dataset(\"dwisaji/indonesia-telecomunication-sentiment-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Konversi kolom label ke ClassLabel\n",
    "label_names = ['Negatif', 'Netral', 'Positif']\n",
    "ds = ds.cast_column('label', ClassLabel(names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split dataset dengan stratifikasi\n",
    "train_val_split = ds['train'].train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "train_data = train_val_split['train']\n",
    "val_data = train_val_split['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 212.02ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27517"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.to_csv('val_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenisasi\n",
    "model_name = \"indolem/indobert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_data.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_train.rename_column('label', 'labels')\n",
    "tokenized_val = tokenized_val.rename_column('label', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_train.remove_columns(['text'])\n",
    "tokenized_val = tokenized_val.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_val.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom tokenized_train: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Kolom tokenized_val: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "\n",
      "Jumlah data training: 1977\n",
      "Jumlah data validasi: 495\n"
     ]
    }
   ],
   "source": [
    "print(\"Kolom tokenized_train:\", tokenized_train.column_names)\n",
    "print(\"Kolom tokenized_val:\", tokenized_val.column_names)\n",
    "print()\n",
    "print(f\"Jumlah data training: {len(tokenized_train)}\")\n",
    "print(f\"Jumlah data validasi: {len(tokenized_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh data training:\n",
      "{'labels': tensor(0), 'input_ids': tensor([    3, 24816,  5805,  3151, 25033,   935,  4143, 23579,   931, 13729,\n",
      "            4]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "Contoh data validasi:\n",
      "{'labels': tensor(2), 'input_ids': tensor([    3,  3353,  1522, 22603,   934,     4]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Contoh data training:\")\n",
    "print(tokenized_train[0])  # Sekarang harus menampilkan data\n",
    "print(\"Contoh data validasi:\")\n",
    "print(tokenized_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 6. Konfigurasi model\n",
    "id_to_labels = {0: 'Positif', 1: 'Netral', 2: 'Negatif'}\n",
    "label_to_id = {'Positif': 0, 'Netral': 1, 'Negatif': 2}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id_to_labels,\n",
    "    label2id=label_to_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh teks: erorrrr\n",
      "Label asli (numerik): 2\n",
      "Label asli (nama): Negatif\n"
     ]
    }
   ],
   "source": [
    "sample = val_data[0]\n",
    "print(f\"Contoh teks: {sample['text']}\")\n",
    "print(f\"Label asli (numerik): {sample['label']}\")\n",
    "print(f\"Label asli (nama): {id_to_labels[sample['label']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)['accuracy']\n",
    "    f1_score = f1.compute(predictions=preds, references=labels, average='weighted')['f1']\n",
    "    return {\"accuracy\": acc, \"f1\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. Konfigurasi training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"indobert-sentiment-3class\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9096\\1850138733.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 10. Inisialisasi Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 44:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917660</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.550422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881260</td>\n",
       "      <td>0.648485</td>\n",
       "      <td>0.591184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.755020</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.645486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.757981</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.656836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>0.759780</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.659129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi pada data validasi:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Evaluasi: {'eval_loss': 0.7597801685333252, 'eval_accuracy': 0.692929292929293, 'eval_f1': 0.6591292103931533, 'eval_runtime': 20.7868, 'eval_samples_per_second': 23.813, 'eval_steps_per_second': 1.491, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# 11. Jalankan training\n",
    "trainer.train()\n",
    "\n",
    "# 12. Evaluasi akhir\n",
    "print(\"Evaluasi pada data validasi:\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"Hasil Evaluasi: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: oi ngerti bahasa indonesia kan pokoknya sampe jam belon beres gua gratis jam mokad gratis dst dst lu gila mati service melulu jam berkali &amp\n",
      "Prediksi: Netral (Confidence: 0.39)\n"
     ]
    }
   ],
   "source": [
    "# Contoh prediksi\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = np.exp(outputs.logits.detach().numpy())\n",
    "    probs = probs / probs.sum(axis=-1, keepdims=True)\n",
    "    pred_id = np.argmax(probs)\n",
    "    return id_to_labels[pred_id], probs[0][pred_id]\n",
    "\n",
    "# Test contoh yang bermasalah\n",
    "text = \"oi ngerti bahasa indonesia kan pokoknya sampe jam belon beres gua gratis jam mokad gratis dst dst lu gila mati service melulu jam berkali &amp\"\n",
    "prediction, confidence = predict(text)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Prediksi: {prediction} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh teks: oi ngerti bahasa indonesia kan pokoknya sampe jam belon beres gua gratis jam mokad gratis dst dst lu gila mati service melulu jam berkali &amp\n",
      "Label asli: Negatif\n",
      "142\n",
      "\n",
      "Text: oi ngerti bahasa indonesia kan pokoknya sampe jam belon beres gua gratis jam mokad gratis dst dst lu gila mati service melulu jam berkali &amp\n",
      "Label Asli: Negatif\n",
      "Prediksi: Netral (Confidence: 0.39)\n"
     ]
    }
   ],
   "source": [
    "# Ambil sampel teks dari dataset validasi\n",
    "sample = val_data[206]  # Contoh pertama\n",
    "text_sample = sample['text']\n",
    "label_sample = sample['label']\n",
    "\n",
    "print(f\"Contoh teks: {text_sample}\")\n",
    "print(f\"Label asli: {id_to_labels[label_sample]}\")\n",
    "\n",
    "\n",
    "prediction, confidence = predict(text_sample)\n",
    "print(len(sample['text']))\n",
    "print(f\"\\nText: {text_sample}\")\n",
    "print(f\"Label Asli: {id_to_labels[label_sample]}\")\n",
    "print(f\"Prediksi: {prediction} (Confidence: {confidence:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluasi data dengan eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = ds['train'].train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "train_data = train_val_split['train']\n",
    "val_data = train_val_split['test']  # Ini adalah data validasi Anda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 495/495 [00:00<00:00, 2577.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_val = val_data.map(preprocess_function, batched=True)\n",
    "tokenized_val = tokenized_val.rename_column('label', 'labels')\n",
    "tokenized_val = tokenized_val.remove_columns(['text'])\n",
    "tokenized_val.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9096\\1025853563.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,  # Gunakan data validasi Anda di sini\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Evaluasi pada Data Validasi: {'eval_loss': 0.7597801685333252, 'eval_model_preparation_time': 0.0081, 'eval_accuracy': 0.692929292929293, 'eval_f1': 0.6591292103931533, 'eval_runtime': 20.987, 'eval_samples_per_second': 23.586, 'eval_steps_per_second': 1.477}\n",
      "Hasil Evaluasi Manual: {'accuracy': 0.692929292929293, 'f1': 0.6591292103931533}\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi pada data validasi\n",
    "results = trainer.evaluate()\n",
    "print(f\"Hasil Evaluasi pada Data Validasi: {results}\")\n",
    "\n",
    "# Jika ingin prediksi manual\n",
    "predictions = trainer.predict(tokenized_val)\n",
    "logits, labels = predictions.predictions, predictions.label_ids\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(f\"Hasil Evaluasi Manual: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
